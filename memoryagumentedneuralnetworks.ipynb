{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XqUgHjh7XAUW"
      },
      "outputs": [],
      "source": [
        "# Load the required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the iris dataset\n",
        "iris = load_iris()\n",
        "\n",
        "# Extract the features and target variable\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Train the logistic regression model\n",
        "logreg = LogisticRegression(multi_class='multinomial', solver='newton-cg')\n",
        "logreg.fit(X, y)\n",
        "\n",
        "# Compute the predicted probabilities for each class\n",
        "pred_probs = logreg.predict_proba(X)\n",
        "\n",
        "# Get the predicted class for each observation\n",
        "predicted_classes = logreg.predict(X)\n",
        "\n",
        "# Calculate the accuracy\n",
        "accuracy = accuracy_score(y, predicted_classes)\n",
        "\n",
        "# Display the accuracy\n",
        "print(f\"Accuracy on the training dataset: {accuracy*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "id": "kSCGS0_LXOA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Load the iris dataset\n",
        "iris = load_iris()\n",
        "\n",
        "# Extract the features and target variable\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# One-hot encode the target variable\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "y_one_hot = encoder.fit_transform(y.reshape(-1, 1))\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_one_hot, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the neural network architecture\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(10, activation='relu', input_shape=(4,)),\n",
        "    tf.keras.layers.Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model with the Adam optimizer and cross-entropy loss function\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model for 100 epochs\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=16, validation_data=(X_test, y_test))\n",
        "\n",
        "# Plot the loss history\n",
        "loss_history = history.history['loss']\n",
        "plt.plot(np.arange(len(loss_history)), loss_history)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "print(f\"Test loss: {test_loss:.4f}\")\n",
        "print(f\"Test accuracy: {test_acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "WRy-nk54XUiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the iris dataset\n",
        "iris = load_iris()\n",
        "\n",
        "# Extract the features and target variable\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Define the logistic ridge regression model\n",
        "logreg = LogisticRegression(penalty='l2', solver='liblinear')\n",
        "\n",
        "# Define the range of the regularization parameter to search over\n",
        "param_grid = {'C': np.logspace(-4, 4, 9)}\n",
        "\n",
        "# Perform a grid search over the range of the regularization parameter\n",
        "grid_search = GridSearchCV(logreg, param_grid, cv=5)\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "# Get the best regularization parameter\n",
        "C_best = grid_search.best_params_['C']\n",
        "\n",
        "# Train the logistic ridge regression model with the best regularization parameter\n",
        "logreg = LogisticRegression(penalty='l2', C=C_best, solver='liblinear')\n",
        "logreg.fit(X, y)\n",
        "\n",
        "# Calculate the accuracy on the training set\n",
        "y_pred = logreg.predict(X)\n",
        "accuracy = accuracy_score(y, y_pred)\n",
        "\n",
        "# Display the accuracy on the training set\n",
        "print(f\"Accuracy on the training set: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "JGB2Lu2pYRIZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "# Load the iris dataset\n",
        "iris = load_iris()\n",
        "\n",
        "# Extract the features and target variable\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Convert the target variable to one-hot encoding\n",
        "lb = LabelBinarizer()\n",
        "y = lb.fit_transform(y)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Reshape the features for use in a convolutional neural network\n",
        "X_train = X_train.reshape(-1, 4, 1, 1)\n",
        "X_val = X_val.reshape(-1, 4, 1, 1)\n",
        "\n",
        "# Define the convolutional neural network architecture\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, (2, 1), activation='relu', input_shape=(4, 1, 1)),\n",
        "    tf.keras.layers.MaxPooling2D((2, 1)),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=50, batch_size=16, validation_data=(X_val, y_val))\n",
        "\n",
        "# Evaluate the model on the training set\n",
        "train_loss, train_acc = model.evaluate(X_train, y_train)\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "val_loss, val_acc = model.evaluate(X_val, y_val)\n",
        "\n",
        "# Display the accuracy on the training and validation sets\n",
        "print(f\"Accuracy on the training set: {train_acc:.4f}\")\n",
        "print(f\"Accuracy on the validation set: {val_acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "UQCvfLf0YvPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "# Load the iris dataset\n",
        "iris = load_iris()\n",
        "\n",
        "# Extract the features and target variable\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Convert the target variable to one-hot encoding\n",
        "lb = LabelBinarizer()\n",
        "y = lb.fit_transform(y)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Reshape the features for use in a convolutional neural network\n",
        "X_train = X_train.reshape(-1, 4, 1, 1)\n",
        "X_val = X_val.reshape(-1, 4, 1, 1)\n",
        "\n",
        "# Define the MACNN architecture\n",
        "class MACNN(tf.keras.Model):\n",
        "    def __init__(self, num_classes, num_memory_slots):\n",
        "        super(MACNN, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.num_memory_slots = num_memory_slots\n",
        "        \n",
        "        # Define the convolutional module\n",
        "        self.conv1 = tf.keras.layers.Conv2D(32, (2, 1), activation='relu', input_shape=(4, 1, 1))\n",
        "        self.pool1 = tf.keras.layers.MaxPooling2D((2, 1))\n",
        "        self.flatten = tf.keras.layers.Flatten()\n",
        "        self.dense1 = tf.keras.layers.Dense(64, activation='relu')\n",
        "        self.dropout = tf.keras.layers.Dropout(0.5)\n",
        "        \n",
        "        # Define the memory module\n",
        "        self.memory_keys = tf.Variable(tf.random.normal((num_memory_slots, 64)), trainable=True)\n",
        "        self.memory_values = tf.Variable(tf.random.normal((num_memory_slots, num_classes)), trainable=True)\n",
        "        \n",
        "    def call(self, inputs):\n",
        "        # Compute the convolutional features\n",
        "        x = self.conv1(inputs)\n",
        "        x = self.pool1(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.dense1(x)\n",
        "        x = self.dropout(x)\n",
        "        \n",
        "        # Compute the memory-based predictions\n",
        "        attention = tf.matmul(x, tf.transpose(self.memory_keys))\n",
        "        attention_weights = tf.nn.softmax(attention, axis=-1)\n",
        "        memory_output = tf.matmul(attention_weights, self.memory_values)\n",
        "        predictions = tf.nn.softmax(memory_output, axis=-1)\n",
        "        \n",
        "        return predictions\n",
        "\n",
        "# Create an instance of the MACNN model\n",
        "model = MACNN(num_classes=3, num_memory_slots=10)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=50, batch_size=16, validation_data=(X_val, y_val))\n",
        "\n",
        "# Evaluate the model on the training set\n",
        "train_loss, train_acc = model.evaluate(X_train, y_train)\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "val_loss, val_acc = model.evaluate(X_val, y_val)\n",
        "\n",
        "# Display the accuracy on the training and validation sets\n",
        "print(f\"Accuracy on the training set: {train_acc:.4f}\")\n",
        "print(f\"Accuracy on the validation set: {val_acc:.4f}\")"
      ],
      "metadata": {
        "id": "-GY0eaReaJu6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}